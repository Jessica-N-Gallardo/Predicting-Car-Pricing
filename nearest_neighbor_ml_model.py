# -*- coding: utf-8 -*-
"""Nearest Neighbor ML Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y2Mlt5DCzBNNY3-SenNy2e7nulokWmqa

# Housekeeping
"""

!pip install itables

import pandas as pd
import numpy as np

from matplotlib import pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.decomposition import PCA

from itables import init_notebook_mode
init_notebook_mode(all_interactive=True)

from google.colab import drive
drive.mount('/content/drive')
drive_dir = '/content/drive/MyDrive/Spring 2025/Data Science/Projects/Project 3: Car Prices Data/Data/'

df = pd.read_csv(drive_dir + 'Final_Data.csv')

"""### Fix some data discrepancies"""

#fix inifinities
df.loc[(df['savings_amount'] == 0) | (df['years_old'] == 0), 'yearly_depreciation'] = 0
df.loc[(df['savings_amount'] != 0) & (df['years_old'] != 0), 'yearly_depreciation'] = df['savings_amount'] / df['years_old']

#sanity check
df['yearly_depreciation'].describe()

#fixing mileage depreciation per mile values
df.loc[(df['savings_amount'] == 0) | (df['mileage'] == 0), 'depreciation_per_mile'] = 0
df.loc[(df['savings_amount'] != 0) & (df['mileage'] != 0), 'depreciation_per_mile'] = df['savings_amount'] / df['mileage']

#sanity check for depreciation_per_mile column
df['depreciation_per_mile'].describe()

#check how many values are negative in years old
df[df['years_old'] == -1]

#drop the negative -1 entries in this years_old
df = df[df['years_old'] != -1]

df.head()

#filter for only the top 20 car makes
#top_20_makes = df['make_name'].value_counts().head(20).index  # Get top 20 car makes
#df = df[df['make_name'].isin(top_20_makes)]  # Filter original DataFrame

"""# Label Encoding"""

df.head()

df.dtypes

#drop vin column - useless column
df = df.drop(['vin' , 'dealer_zip'], axis= 1)

df.head()

df.columns

#list categorical columns

cat_cols = ['body_type', 'engine_type', 'fleet', 'frame_damaged', 'franchise_dealer',
            'fuel_type', 'has_accidents', 'interior_color', 'iscab', 'listing_color', 'make_name',
            'model_name', 'salvage',  'sp_name', 'theft_title', 'transmission','wheel_system_display']

label_encoder = {}

#create for loop to go through each categorical column

for c in cat_cols:
  #create a new label encoder for each column
  le = LabelEncoder()

  #fit the label encoder on the column data
  le.fit(df[c])

  #transform the column to numeric
  df[c] = le.transform(df[c])

  #save label encoder to label_encoder dictionary, indexed by the column
  label_encoder[c] = le

df.head()

"""# Modeling

## Splitting Data
"""

df.columns

#split data: train - 80%, test - 20%
train_df, test_df = train_test_split(df, train_size = .8 , test_size = .2)
X_train = train_df[['body_type', 'daysonmarket', 'engine_displacement',
       'engine_type', 'fleet', 'frame_damaged', 'franchise_dealer',
       'front_legroom', 'fuel_tank_volume', 'fuel_type', 'has_accidents',
       'height', 'horsepower', 'interior_color', 'iscab', 'length',
       'listing_color', 'make_name', 'maximum_seating', 'mileage',
       'model_name', 'owner_count', 'salvage', 'savings_amount',
       'sp_name', 'theft_title', 'transmission', 'wheel_system_display',
       'wheelbase', 'width', 'year', 'years_old', 'option_count',
       'original_price', 'yearly_depreciation', 'depreciation_per_mile']]
y_train = train_df['price']

X_test = test_df[['body_type', 'daysonmarket', 'engine_displacement',
       'engine_type', 'fleet', 'frame_damaged', 'franchise_dealer',
       'front_legroom', 'fuel_tank_volume', 'fuel_type', 'has_accidents',
       'height', 'horsepower', 'interior_color', 'iscab', 'length',
       'listing_color', 'make_name', 'maximum_seating', 'mileage',
       'model_name', 'owner_count', 'salvage', 'savings_amount',
       'sp_name', 'theft_title', 'transmission', 'wheel_system_display',
       'wheelbase', 'width', 'year', 'years_old', 'option_count',
       'original_price', 'yearly_depreciation', 'depreciation_per_mile']]
y_test = test_df['price']

"""## Scaling"""

#we need to scale our data first
scaler = StandardScaler()

#fit and transform train dataset
X_train_scaled = scaler.fit_transform(X_train)

#fit and transform test dataset
X_test_scaled = scaler.fit_transform(X_test)

"""## KNeighborsRegressor Model"""

#call in KNN Regressor because we have both categorical and continuous variables
knn_regr = KNeighborsRegressor()

#fit the model
knn_regr.fit(X_train_scaled , y_train)

#predict
knn_regr_preds = knn_regr.predict(X_test_scaled)

#these are our regression metrics

MAE = mean_absolute_error(y_test, knn_regr_preds) #measures the absolute difference between our predictions and the real y values (car prices)
MSE = mean_squared_error(y_test , knn_regr_preds) #^^ same thing but squared
R2 = r2_score(y_test , knn_regr_preds)

print(f"mean_absolute_error: {MAE}")
print(f"mean_squared_error:{MSE}")
print(f"R^2: {R2}")

#Let's see how we did.
plt.figure(figsize = (5, 9))
plt.scatter(y_test , knn_regr_preds)
plt.title('Actual vs. Predictions')
plt.xlabel('Actual Values')
plt.ylabel('Predictions')

"""## Removing Outliers - only 10% of data"""

#filter for only the top 20 car makes
top_20_makes = df['make_name'].value_counts().head(20).index  # Get top 20 car makes
df = df[df['make_name'].isin(top_20_makes)]  # Filter original DataFrame

"""### Price Predictions"""

#split data: train - 80%, test - 20%
train_df, test_df = train_test_split(df, train_size = .8 , test_size = .2)
X_train = train_df[['body_type', 'daysonmarket', 'engine_displacement',
       'engine_type', 'fleet', 'frame_damaged', 'franchise_dealer',
       'front_legroom', 'fuel_tank_volume', 'fuel_type', 'has_accidents',
       'height', 'horsepower', 'interior_color', 'iscab', 'length',
       'listing_color', 'make_name', 'maximum_seating', 'mileage',
       'model_name', 'owner_count', 'salvage',
       'sp_name', 'theft_title', 'transmission', 'wheel_system_display',
       'wheelbase', 'width', 'year', 'years_old', 'option_count',
       'original_price', 'yearly_depreciation', 'depreciation_per_mile']]
y_train = train_df['price']

X_test = test_df[['body_type', 'daysonmarket', 'engine_displacement',
       'engine_type', 'fleet', 'frame_damaged', 'franchise_dealer',
       'front_legroom', 'fuel_tank_volume', 'fuel_type', 'has_accidents',
       'height', 'horsepower', 'interior_color', 'iscab', 'length',
       'listing_color', 'make_name', 'maximum_seating', 'mileage',
       'model_name', 'owner_count', 'salvage',
       'sp_name', 'theft_title', 'transmission', 'wheel_system_display',
       'wheelbase', 'width', 'year', 'years_old', 'option_count',
       'original_price', 'yearly_depreciation', 'depreciation_per_mile']]
y_test = test_df['price']

#all columns except we didn't include savings_amount column

#we need to scale our data first
scaler = StandardScaler()

#fit and transform train dataset
X_train_scaled = scaler.fit_transform(X_train)

#fit and transform test dataset
X_test_scaled = scaler.fit_transform(X_test)

#call in KNN Regressor because we have both categorical and continuous variables
knn_regr = KNeighborsRegressor()

#fit the model
knn_regr.fit(X_train_scaled , y_train)

#predict
knn_regr_preds = knn_regr.predict(X_test_scaled)

#these are our regression metrics

MAE = mean_absolute_error(y_test, knn_regr_preds) #measures the absolute difference between our predictions and the real y values (car prices)
MSE = mean_squared_error(y_test , knn_regr_preds) #^^ same thing but squared
R2 = r2_score(y_test , knn_regr_preds)

print(f"mean_absolute_error: {MAE}")
print(f"mean_squared_error:{MSE}")
print(f"R^2: {R2}")

#let's check our results again.

plt.figure(figsize = (5, 9))
plt.scatter(y_test , knn_regr_preds)
plt.title('Actual vs. Predictions')
plt.xlabel('Actual Values')
plt.ylabel('Predictions')

"""# Hyperparameter Tuning"""

#I need to look at which columns are the weakest and strongest via hyperparameter tuning
# I need to also look at distance metrics

#define hyperparameter grid
param_grid = {
    'n_neighbors': range(1,21),
    'weights': ['distance', 'uniform'],
    'p': [2]
}

grid_search = GridSearchCV(
    estimator=KNeighborsRegressor(),
    param_grid=param_grid,
    cv=5,  # 5-fold cross-validation
    scoring='neg_mean_absolute_error',  # You can also use 'r2'
    n_jobs=-1  # Use all CPU cores for speed
)

grid_search.fit(X_train_scaled, y_train)

print("Best Parameters:", grid_search.best_params_)

best_k = grid_search.best_params_['n_neighbors']
best_weights = grid_search.best_params_['weights']
best_p = grid_search.best_params_['p']

# Re-train the best model
knn_reg_best = KNeighborsRegressor(n_neighbors=best_k, weights=best_weights, p=best_p)
knn_reg_best.fit(X_train_scaled, y_train)

# Predict on test data
knn_preds_best = knn_reg_best.predict(X_test_scaled)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

mae = mean_absolute_error(y_test, knn_preds_best)
mse = mean_squared_error(y_test, knn_preds_best)
r2 = r2_score(y_test, knn_preds_best)

print(f"Tuned MAE: {mae}")
print(f"Tuned MSE: {mse}")
print(f"Tuned RÂ²: {r2}")

#let's check our results again.

plt.figure(figsize = (5, 9))
plt.scatter(y_test , knn_preds_best)
plt.title('Actual vs. Predictions')
plt.xlabel('Actual Values')
plt.ylabel('Predictions')

"""### Days on market Predictions"""

#split data: train - 80%, test - 20%
train_df, test_df = train_test_split(df, train_size = .8 , test_size = .2)
X_train = train_df[['body_type', 'price','engine_displacement',
       'engine_type', 'fleet', 'frame_damaged', 'franchise_dealer',
       'front_legroom', 'fuel_tank_volume', 'fuel_type', 'has_accidents',
       'height', 'horsepower', 'interior_color', 'iscab', 'length',
       'listing_color', 'make_name', 'maximum_seating', 'mileage',
       'model_name', 'owner_count', 'salvage', 'savings_amount',
       'sp_name', 'theft_title', 'transmission', 'wheel_system_display',
       'wheelbase', 'width', 'year', 'years_old', 'option_count',
       'original_price', 'yearly_depreciation', 'depreciation_per_mile']]
y_train = train_df['daysonmarket']

X_test = test_df[['body_type', 'price', 'engine_displacement',
       'engine_type', 'fleet', 'frame_damaged', 'franchise_dealer',
       'front_legroom', 'fuel_tank_volume', 'fuel_type', 'has_accidents',
       'height', 'horsepower', 'interior_color', 'iscab', 'length',
       'listing_color', 'make_name', 'maximum_seating', 'mileage',
       'model_name', 'owner_count', 'salvage', 'savings_amount',
       'sp_name', 'theft_title', 'transmission', 'wheel_system_display',
       'wheelbase', 'width', 'year', 'years_old', 'option_count',
       'original_price', 'yearly_depreciation', 'depreciation_per_mile']]
y_test = test_df['daysonmarket']

knn_days_regr = KNeighborsRegressor()

#fit the model
knn_days_regr.fit(X_train , y_train)

#predict
knn_days_regr_preds = knn_days_regr.predict(X_test)

